defaults:
- override hydra/job_logging: default
- override hydra/hydra_logging: default

hydra:
  run:
    dir: /root/autodl-tmp/outputs/${now:%m-%d}/${now:%H-%M-%S}
    #dir: ./outputs/${now:%m-%d}/${now:%H-%M-%S}

name: autodl_b32_h128_e128_1e10_teacher_batch16_k3_min_1536
#run_id: 20ovj7hu
# teacher: autodl_b16_h128_e32_5e9_teacher_batch16

## WANDB params
# The wandb project name
project: threes-ai
# The wandb user to log to
entity: pyflynn
# The wandb group for the run
group: autodl_teacher_runs

## ENV params
obs_space_kwargs: {}
reward_space_kwargs: {}

## TRAINING params
total_steps: 1e10
num_actors: 15
n_actor_envs: 256
unroll_length: 16
batch_size: 128
discounting: 0.999

## MODEL params
model_arch: conv_model
n_blocks: 32
hidden_dim: 128
embedding_dim: 128
n_merge_layers: 1
normalize: False
sum_player_embeddings: False
use_index_select: False
rescale_value_input: True
rescale_se_input: True
# Conv-specific params
kernel_size: 3


## OPTIMIZER params
optimizer_class: Adam
optimizer_kwargs:
  lr: 5e-5
  # See https://arxiv.org/pdf/2105.05246.pdf
  eps: 0.0003
  #alpha: 0.9
min_lr_mod: 0.01

## LOSS params
entropy_cost: 0.0002
baseline_cost: 1.
teacher_kl_cost: 0.005
teacher_baseline_cost: 0.0
# lambda parameter for TD-lambda and UPGO losses
lmb: 0.9
reduction: sum

# Pretrained model for KL loss
use_teacher: True
#teacher_load_dir: /root/autodl-tmp/outputs/11-26/08-59-57/
#teacher_checkpoint_file: 1613443072_weights.pt
#teacher_load_dir: /Users/flynn.wang/repo/flynn/thress_imgs/models/move_model/autodl_b16_h128_e32_5e9_teacher_batch16_1130
#teacher_checkpoint_file: 0527366656_weights.pt
#
#teacher_load_dir: /root/autodl-tmp/outputs/11-27/23-28-57/
#teacher_checkpoint_file: 1182961664_weights.pt
#
#teacher_load_dir: /root/autodl-tmp/outputs/11-29/14-37-29/
#teacher_checkpoint_file: 0682200320_weights.pt
#
#teacher_load_dir: /root/autodl-nas/snapshot/autodl_b32_h128_e128_5e9_teacher_batch16_kernel3_cont
#teacher_checkpoint_file: 1790685184_weights.pt

#
#teacher_load_dir: /root/autodl-nas/snapshot/autodl_b64_h128_e128_5e9_teacher_batch16_kernel3_cont
#teacher_checkpoint_file: 4855656448_weights.pt
#
#teacher_load_dir: /root/autodl-nas/snapshot/autodl_b32_h128_e128_1e10_teacher_batch16_k3_1228_half_max_card_penalty
#teacher_checkpoint_file: 00380946432_weights.pt
#
teacher_load_dir: /root/autodl-nas/snapshot/autodl_b32_h128_e128_1e10_teacher_batch16_k3_half_max_card_penalty_1229
teacher_checkpoint_file: 00387893248_weights.pt

# MISCELLANEOUS params
use_mixed_precision: True
actor_device: cuda
learner_device: cuda
#actor_device: cpu
#learner_device: cpu
model_log_freq: 300
# file_descriptor or file_system
sharing_strategy: file_system
disable_wandb: False
#disable_wandb: True
debug: False

# restart config
#load_dir: /root/autodl-tmp/outputs/12-02/14-10-50
#checkpoint_file: 0692463616.pt

#load_dir: /root/autodl-tmp/outputs/12-04/23-08-20
#checkpoint_file: 0331423744.pt
#checkpoint_file: 0331423744.pt
#
#load_dir: /root/autodl-tmp/outputs/12-05/22-27-22
#checkpoint_file: 2539980800.pt
#
#load_dir: /root/autodl-tmp/outputs/12-12/09-02-42/
#load_dir: /root/autodl-nas/snapshot/autodl_b64_h128_e128_5e9_teacher_batch16_kernel3_cont
#checkpoint_file: 4855656448.pt
#
#load_dir: /root/autodl-tmp/outputs/12-25/21-03-39
#checkpoint_file: 01246488576.pt

#load_dir: /root/autodl-nas/snapshot/autodl_b32_h128_e128_1e10_teacher_batch16_k3_half_max_card_penalty_1229_v2
#checkpoint_file: 00294414336.pt
#
load_dir: /root/autodl-nas/snapshot/autodl_b32_h128_e128_1e10_teacher_batch16_k3_min_1536/
checkpoint_file: 00813164544.pt

weights_only: False
n_value_warmup_batches: 600
